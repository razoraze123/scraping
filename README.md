ğŸ¯ Objectif
Ce projet vise Ã  fournir un script Python de scraping flexible et Ã©volutif, permettant d'extraire automatiquement toutes les informations clÃ©s des fiches produits depuis des sites e-commerce sous WooCommerce ou Shopify.

ğŸš€ FonctionnalitÃ©s
Extraction des fiches produits : Nom, lien, type (simple/variable), variantes (tailles, couleursâ€¦).

RÃ©cupÃ©ration de toutes les images (carrousel inclus), avec tÃ©lÃ©chargement local dans un dossier par produit.

Extraction de la description complÃ¨te de chaque produit (sauvegardÃ©e en .txt).

Exploration automatique des pages de collection/catÃ©gorie, rÃ©cupÃ©ration du nom et lien de chaque produit.

DÃ©tection automatique du CMS (WooCommerce ou Shopify).

Gestion de la pagination (statique ou dynamique).

VÃ©rification du bon tÃ©lÃ©chargement des images.

Double mode :

Statique : requests + BeautifulSoup (sites simples)

Dynamique : Selenium (JS/carrousels/pagination dynamique)

> **Note** : le mode dynamique nÃ©cessite Chrome/Chromedriver installÃ©s.

Format de sortie au choix : CSV, Excel (.xlsx), JSON.

ğŸ“¦ Structure des dossiers & fichiers
bash
Copier
ecom_scraper/
â”œâ”€â”€ scraper.py
â”œâ”€â”€ config.yaml
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ /outputs/
â”‚   â”œâ”€â”€ produits.csv|xlsx|json
â”‚   â”œâ”€â”€ descriptions/
â”‚   â”‚    â””â”€â”€ {nom_produit}.txt
â”‚   â””â”€â”€ images/
â”‚        â””â”€â”€ {nom_produit}/
â”‚             â”œâ”€â”€ img1.jpg
â”‚             â””â”€â”€ img2.jpg
â””â”€â”€ /utils/
     â””â”€â”€ ...
âš™ï¸ Installation
Cloner le dÃ©pÃ´t

bash
Copier
git clone https://github.com/ton-utilisateur/ecom_scraper.git
cd ecom_scraper
CrÃ©er un environnement virtuel (optionnel mais recommandÃ©)

bash
Copier
python -m venv venv
source venv/bin/activate  # ou .\venv\Scripts\activate sous Windows
Installer les dÃ©pendances

bash
Copier
pip install -r requirements.txt
Installer Chrome et Chromedriver (pour le mode dynamique)

Vous pouvez installer Chrome depuis <https://www.google.com/chrome/> et telecharger Chromedriver depuis <https://chromedriver.chromium.org/downloads>. Assurez-vous que la version de Chromedriver corresponde a celle de Chrome et que l'exÃ©cutable soit dans votre PATH.

ğŸ› ï¸ Utilisation
1. Configurer config.yaml
Renseigner lâ€™URL de la page de collection Ã  scraper

Exemple minimal :

```yaml
url: "https://exemple.com/collection"
mode: "static"  # ou "dynamic"
output_format: "csv"
output_dir: "outputs"
headless: true
```

Choisir le mode : statique ou dynamique

SÃ©lectionner le format de sortie : csv, xlsx, json

DÃ©finir les chemins de sortie

2. Lancer le script
bash
Copier
python -m ecom_scraper.scraper --config config.yaml
3. RÃ©sultats
Un fichier de sortie au format choisi (outputs/produits.csv|xlsx|json)

Toutes les images tÃ©lÃ©chargÃ©es dans /outputs/images/{nom_produit}/

Les descriptions dans /outputs/descriptions/{nom_produit}.txt

ğŸ“‹ Format de sortie
nom_du_produit	lien_produit	type_produit	variantes	images (liste URL)	description (.txt)
...	...	...	...	...	...

âœ¨ Exemples dâ€™utilisation
Des fichiers de configuration types et des rÃ©sultats d'exemple seront ajoutÃ©s ultÃ©rieurement.

ğŸ§© TODO & Roadmap
 Interface graphique (PySide6)

 Mode batch multi-URL

 Gestion avancÃ©e des proxies/cookies/auth

 Ajout du scraping des avis clients

 AmÃ©lioration de la gestion des erreurs

ğŸ“ Contribution
Fork, PR bienvenues !

Pour toute suggestion : issues GitHub

ğŸ“„ Licence
MIT (ou ce que tu veux)

â• Remarques pour Codex
Penser Ã  la structure modulaire pour faciliter la future intÃ©gration dâ€™une interface.

Le script doit Ãªtre documentÃ© (docstrings, commentaires, logs propres).

PrÃ©voir un systÃ¨me de logs (basique).

SÃ©parer clairement le code par type de site (WooCommerce vs Shopify).

Penser au paramÃ©trage flexible via config.yaml.
